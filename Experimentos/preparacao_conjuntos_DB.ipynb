{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558c22d2",
   "metadata": {},
   "source": [
    "## Preparação dos Conjuntos de Dados\n",
    "Para conduzir os testes da atualização incremental e em lote, foram desenvolvidas abordagens distintas na preparação dos conjuntos de dados.  O objetivo foi avaliar o desempenho dessas abordagens na evolução do esquema JSON em diferentes contextos, nos quais os experimentos podem variar quanto ao conjunto de dados utilizado, ao tamanho dos incrementos (quantidade de novos documentos) e à quantidade de atributos por documento (e.g. baixa, média ou alta quantidade de atributos por documento). A seguir, são detalhadas as etapas dessas preparações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa352d7",
   "metadata": {},
   "source": [
    "## Preparação dos Dados para Abordagem JSD Evolution\n",
    "\n",
    "Para os experimentos de evolução incremental, utilizou-se uma progressão geométrica (PG) para dividir as coleções em incrementos de diferentes tamanhos, representando os novos documentos usados para evoluir o esquema JSON. O primeiro incremento foi utilizado para criar o esquema inicial, enquanto os incrementos seguintes foram utilizados para evoluir o esquema atual. Esses incrementos refletem diferentes estágios de crescimento da quantidade de documentos. Por exemplo, é possível observar como o esquema inicial evolui conforme mais documentos são adicionados e como essa evolução afeta o desempenho em termos de tempo de processamento.\n",
    "\n",
    "Para determinar o primeiro termo (`Dn₁`) e a razão (`r`) da progressão geométrica (PG), foram utilizados apenas números inteiros. Dessa forma, a distribuição dos documentos em incrementos resultou em quantidades inteiras. O conjunto de dados do **Twitter** foi particionado usando uma PG, com primeiro termo de **450** (`Dn₁ = 450`) e razão igual a **2**, simulando incrementos progressivos na quantidade de documentos. Esse conjunto de dados foi dividido considerando a quantidade de documentos da coleção. A soma dos **12** primeiros termos (incrementos `Dnᵢ`, com `i` variando de 1 a `m`, onde `m = 12`) dessa PG resulta em **1.872.500 documentos**.\n",
    "\n",
    "Para o conjunto de dados **VK**, foram adotados os mesmos valores da PG usados para particionar a coleção de documentos do Twitter (primeiro termo de **450** e razão **2**). A variação do tamanho dos incrementos, proporcional à PG, permite analisar como as abordagens de evolução incremental e em lote se comportam com mesma quantidade de documentos, mas com alta e média quantidade de atributos por documento (Twitter e VK, respectivamente).\n",
    "\n",
    "No caso do conjunto de dados **Livros**, foi adotada uma PG com primeiro termo igual a **10.000** (`Dn₁ = 10.000`) e razão **2**, refletindo o crescimento progressivo da quantidade de documentos. Os valores da PG foram escolhidos conforme o tamanho de cada conjunto de dados, visando assegurar uma representação adequada do crescimento progressivo do número de documentos em cada coleção.\n",
    "\n",
    "A Figura abaixo ilustra como a PG foi usada para dividir as coleções em incrementos de diferentes tamanhos.\n",
    "\n",
    "![Particionamento das coleções em incrementos de diferentes tamanhos](./imagem/JSD_E.png)\n",
    "*Figura 1 — Particionamento das coleções em incrementos de diferentes tamanhos.*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Coleção usada no esquema inicial\n",
    "- `twitter_450`\n",
    "- `vk_450`\n",
    "- `livros_10000`\n",
    "\n",
    "### Conjuntos de coleções usadas para atualização\n",
    "\n",
    "#### Conjunto de dados Twitter\n",
    "`twitter_900`, `twitter_1800`, `twitter_3600`, `twitter_7200`, `twitter_14400`,  \n",
    "`twitter_28800`, `twitter_57600`, `twitter_115200`, `twitter_230400`,  \n",
    "`twitter_460800`, `twitter_921600`\n",
    "\n",
    "#### Conjunto de dados VK\n",
    "`vk_900`, `vk_1800`, `vk_3600`, `vk_7200`, `vk_14400`,  \n",
    "`vk_28800`, `vk_57600`, `vk_115200`, `vk_230400`,  \n",
    "`vk_460800`, `vk_921600`\n",
    "\n",
    "#### Conjunto de dados de metadados de livros\n",
    "`books_10000`, `books_20000`, `books_40000`, `books_80000`,  \n",
    "`books_160000`, `books_320000`, `books_640000`, `books_1280000`,  \n",
    "`books_2560000`, `books_5120000`, `books_10240000`, `books_20480000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d70bc7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vk_450: esperado 1-450 (tamanho 450), inseridos 450\n",
      "vk_900: esperado 451-1350 (tamanho 900), inseridos 900\n",
      "vk_1800: esperado 1351-3150 (tamanho 1800), inseridos 1800\n",
      "vk_3600: esperado 3151-6750 (tamanho 3600), inseridos 3600\n",
      "vk_7200: esperado 6751-13950 (tamanho 7200), inseridos 7200\n",
      "vk_14400: esperado 13951-28350 (tamanho 14400), inseridos 14400\n",
      "vk_28800: esperado 28351-57150 (tamanho 28800), inseridos 28800\n",
      "vk_57600: esperado 57151-114750 (tamanho 57600), inseridos 57600\n",
      "vk_115200: esperado 114751-229950 (tamanho 115200), inseridos 115200\n",
      "vk_230400: esperado 229951-460350 (tamanho 230400), inseridos 230400\n",
      "vk_460800: esperado 460351-921150 (tamanho 460800), inseridos 460800\n",
      "vk_921600: esperado 921151-1842750 (tamanho 921600), inseridos 921600\n",
      "Total percorrido no cursor: 1842750 documentos (coleção ~3036654).\n",
      "Particionamento concluído.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Particionamento disjunto por PG (sem skip), para grandes volumes.\n",
    "Gera coleções no formato <out_prefix><tamanho>, ex.: vk_450, vk_900, ...\n",
    "\n",
    "Uso:\n",
    "    python partition_pg_disjoint.py [config.json] [dataset]\n",
    "Ex.:\n",
    "    python partition_pg_disjoint.py config.json vk\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def termos_pg(a1: int, r: int, m: int):\n",
    "    vals, v = [], a1\n",
    "    for _ in range(m):\n",
    "        vals.append(v)\n",
    "        v *= r\n",
    "    return vals\n",
    "\n",
    "\n",
    "def load_dataset_cfg(path: str, dataset: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = json.load(f)\n",
    "\n",
    "    if \"mongo\" not in cfg or \"uri\" not in cfg[\"mongo\"]:\n",
    "        raise ValueError(\"Config inválida: faltou mongo.uri\")\n",
    "    if \"datasets\" not in cfg or dataset not in cfg[\"datasets\"]:\n",
    "        raise ValueError(f\"Config inválida: faltou datasets.{dataset}\")\n",
    "\n",
    "    ds = cfg[\"datasets\"][dataset]\n",
    "    if \"partitioning\" not in ds:\n",
    "        raise ValueError(f\"Config inválida: faltou datasets.{dataset}.partitioning\")\n",
    "\n",
    "    p = ds[\"partitioning\"]\n",
    "    for key in [\"db_name\", \"src_collection\", \"out_prefix\", \"pg\", \"performance\"]:\n",
    "        if key not in p:\n",
    "            raise ValueError(f\"Config inválida: faltou partitioning.{key}\")\n",
    "    for key in [\"dn1\", \"r\", \"m\"]:\n",
    "        if key not in p[\"pg\"]:\n",
    "            raise ValueError(f\"Config inválida: faltou partitioning.pg.{key}\")\n",
    "    for key in [\"read_batch_size\", \"write_chunk_size\"]:\n",
    "        if key not in p[\"performance\"]:\n",
    "            raise ValueError(f\"Config inválida: faltou partitioning.performance.{key}\")\n",
    "\n",
    "    return {\n",
    "        \"MONGO_URI\": cfg[\"mongo\"][\"uri\"],\n",
    "        \"DB_NAME\": p[\"db_name\"],\n",
    "        \"SRC_COLL\": p[\"src_collection\"],\n",
    "        \"OUT_PREFIX\": p[\"out_prefix\"],\n",
    "        \"Dn1\": p[\"pg\"][\"dn1\"],\n",
    "        \"r\": p[\"pg\"][\"r\"],\n",
    "        \"m\": p[\"pg\"][\"m\"],\n",
    "        \"READ_BATCH_SIZE\": p[\"performance\"][\"read_batch_size\"],\n",
    "        \"WRITE_CHUNK_SIZE\": p[\"performance\"][\"write_chunk_size\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    config_path =  \"config.json\"\n",
    "    dataset =  \"vk\"\n",
    "\n",
    "    cfg = load_dataset_cfg(config_path, dataset)\n",
    "    MONGO_URI = cfg[\"MONGO_URI\"]\n",
    "    DB_NAME = cfg[\"DB_NAME\"]\n",
    "    SRC_COLL = cfg[\"SRC_COLL\"]\n",
    "    OUT_PREFIX = cfg[\"OUT_PREFIX\"]\n",
    "    Dn1, r, m = cfg[\"Dn1\"], cfg[\"r\"], cfg[\"m\"]\n",
    "    READ_BATCH_SIZE = cfg[\"READ_BATCH_SIZE\"]\n",
    "    WRITE_CHUNK_SIZE = cfg[\"WRITE_CHUNK_SIZE\"]\n",
    "    #dataset = cfg[\"default_dataset\"]\n",
    "\n",
    "    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
    "    client.admin.command(\"ping\")\n",
    "    db = client[DB_NAME]\n",
    "\n",
    "    if SRC_COLL not in db.list_collection_names():\n",
    "        raise RuntimeError(\n",
    "            f\"Coleção fonte '{SRC_COLL}' não existe no DB '{DB_NAME}'. \"\n",
    "            f\"Existentes: {db.list_collection_names()}\"\n",
    "        )\n",
    "\n",
    "    source = db[SRC_COLL]\n",
    "    total_docs = source.estimated_document_count()\n",
    "    if total_docs == 0:\n",
    "        raise RuntimeError(f\"A coleção '{SRC_COLL}' está vazia.\")\n",
    "\n",
    "    part_sizes = termos_pg(Dn1, r, m)\n",
    "    soma_pg = sum(part_sizes)\n",
    "    if soma_pg > total_docs:\n",
    "        print(f\"[aviso] Soma dos tamanhos ({soma_pg}) > total de docs ({total_docs}). \"\n",
    "              \"Partições finais poderão ficar menores/vazias.\")\n",
    "\n",
    "    # prepara coleções de saída\n",
    "    outs, out_names = [], []\n",
    "    for size in part_sizes:\n",
    "        name = f\"{OUT_PREFIX}{size}\"  # ex.: vk_450, vk_900, ...\n",
    "        db[name].drop()\n",
    "        outs.append(db[name])\n",
    "        out_names.append(name)\n",
    "\n",
    "    # uma passada só, ordenado por _id\n",
    "    cursor = source.find({}, projection=None).sort([(\"_id\", 1)]).batch_size(READ_BATCH_SIZE)\n",
    "\n",
    "    idx_part = 0\n",
    "    remaining_in_part = part_sizes[0]\n",
    "    inserted_counts = [0] * len(outs)\n",
    "    total_seen = 0\n",
    "    write_buffer = []\n",
    "\n",
    "    def flush_buffer():\n",
    "        nonlocal write_buffer, inserted_counts, idx_part\n",
    "        if write_buffer:\n",
    "            outs[idx_part].insert_many(\n",
    "                write_buffer, ordered=False, bypass_document_validation=True\n",
    "            )\n",
    "            inserted_counts[idx_part] += len(write_buffer)\n",
    "            write_buffer.clear()\n",
    "\n",
    "    for doc in cursor:\n",
    "        total_seen += 1\n",
    "        write_buffer.append(doc)\n",
    "        remaining_in_part -= 1\n",
    "\n",
    "        if len(write_buffer) >= WRITE_CHUNK_SIZE and remaining_in_part > 0:\n",
    "            flush_buffer()\n",
    "\n",
    "        if remaining_in_part == 0:\n",
    "            flush_buffer()\n",
    "            idx_part += 1\n",
    "            if idx_part >= len(outs):\n",
    "                break\n",
    "            remaining_in_part = part_sizes[idx_part]\n",
    "\n",
    "    if idx_part < len(outs):\n",
    "        flush_buffer()\n",
    "\n",
    "    # logs humanos (faixas 1-based esperadas)\n",
    "    start_h = 1\n",
    "    for size, name, count in zip(part_sizes, out_names, inserted_counts):\n",
    "        end_h = start_h + size - 1\n",
    "        print(f\"{name}: esperado {start_h}-{end_h} (tamanho {size}), inseridos {count}\")\n",
    "        start_h = end_h + 1\n",
    "\n",
    "    print(f\"Total percorrido no cursor: {total_seen} documentos (coleção ~{total_docs}).\")\n",
    "    print(\"Particionamento concluído.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72aa31",
   "metadata": {},
   "source": [
    "## Preparação dos Dados para Abordagem em Lote\n",
    "\n",
    "Adotou-se a estratégia de união dos incrementos particionados durante a preparação dos experimentos na abordagem de evolução incremental, a fim de refletir a natureza do processamento em lote, no qual as atualizações consideram a totalidade dos documentos acumulados até o momento (documentos novos e antigos).\n",
    "\n",
    "![Preparação dos conjuntos de dados da abordagem em lote](./imagem/JSD.png)  \n",
    "*Figura 2 — Preparação dos conjuntos de dados da abordagem em lote.*\n",
    "\n",
    "Seja `Pᵢ` o conjunto de documentos da partição `i`, tal que `i` varia de `1` a `m`, sendo `m` o número total de partições geradas pela PG. Então, para cada partição `Pᵢ`, tem-se:\n",
    "\n",
    "- **P₁**: Partição inicial contendo `Dn₁`, onde `Dn₁` é o primeiro termo da PG, ou seja, representa o primeiro conjunto de dados particionado para os testes da evolução incremental.  \n",
    "- **P₂**: `P₁ ∪ Dn₂`, onde `Dn₂` é o segundo termo da PG e representa o segundo conjunto de dados particionado para os testes da evolução incremental.  \n",
    "- **P₃**: `P₂ ∪ Dn₃`, onde `Dn₃` é o terceiro termo da PG e representa o terceiro conjunto de dados particionado para os testes da evolução incremental.  \n",
    "- **Pᵢ₊₁**: `Pᵢ ∪ Dnᵢ₊₁`, onde `Dnᵢ₊₁` é o próximo termo da PG.\n",
    "\n",
    "A Figura 2 ilustra como cada partição representa a união dos documentos das partições anteriores, simulando o crescimento gradual e cumulativo do conjunto de dados em cenários de evolução em lote. Essa abordagem precisa ser aplicada a todos os conjuntos de dados, tanto novos quanto antigos.\n",
    "\n",
    "---\n",
    "\n",
    "### Conjuntos de coleções usadas na abordagem em Lote\n",
    "\n",
    "#### Conjunto de dados Twitter\n",
    "`twitter_450`, `twitter_1350`, `twitter_3150`, `twitter_6750`,  \n",
    "`twitter_13950`, `twitter_28350`, `twitter_57150`, `twitter_114750`,  \n",
    "`twitter_229950`, `twitter_460350`, `twitter_921150`, `twitter_1842750`\n",
    "\n",
    "#### Conjunto de dados VK\n",
    "`vk_450`, `vk_1350`, `vk_3150`, `vk_6750`,  \n",
    "`vk_13950`, `vk_28350`, `vk_57150`, `vk_114750`,  \n",
    "`vk_229950`, `vk_460350`, `vk_921150`, `vk_1842750`\n",
    "\n",
    "#### Conjunto de dados de metadados de livros\n",
    "`books_10000`, `books_30000`, `books_70000`, `books_150000`,  \n",
    "`books_310000`, `books_630000`, `books_1270000`, `books_2550000`,  \n",
    "`books_5110000`, `books_10230000`, `books_20470000`, `books_40950000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a42df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1: vk.vk_450 -> vk_batch.vk_batch_1  (copiados 450 docs)\n",
      "P2: (vk_batch.vk_batch_1 ∪ vk.vk_900) -> vk_batch.vk_batch_2  (anexados 900, total 1350)\n",
      "P3: (vk_batch.vk_batch_2 ∪ vk.vk_1800) -> vk_batch.vk_batch_3  (anexados 1800, total 3150)\n",
      "P4: (vk_batch.vk_batch_3 ∪ vk.vk_3600) -> vk_batch.vk_batch_4  (anexados 3600, total 6750)\n",
      "P5: (vk_batch.vk_batch_4 ∪ vk.vk_7200) -> vk_batch.vk_batch_5  (anexados 7200, total 13950)\n",
      "P6: (vk_batch.vk_batch_5 ∪ vk.vk_14400) -> vk_batch.vk_batch_6  (anexados 14400, total 28350)\n",
      "P7: (vk_batch.vk_batch_6 ∪ vk.vk_28800) -> vk_batch.vk_batch_7  (anexados 28800, total 57150)\n",
      "P8: (vk_batch.vk_batch_7 ∪ vk.vk_57600) -> vk_batch.vk_batch_8  (anexados 57600, total 114750)\n",
      "P9: (vk_batch.vk_batch_8 ∪ vk.vk_115200) -> vk_batch.vk_batch_9  (anexados 115200, total 229950)\n",
      "P10: (vk_batch.vk_batch_9 ∪ vk.vk_230400) -> vk_batch.vk_batch_10  (anexados 230400, total 460350)\n",
      "P11: (vk_batch.vk_batch_10 ∪ vk.vk_460800) -> vk_batch.vk_batch_11  (anexados 460800, total 921150)\n",
      "P12: (vk_batch.vk_batch_11 ∪ vk.vk_921600) -> vk_batch.vk_batch_12  (anexados 921600, total 1842750)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3 aqui\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import sys\n",
    "from typing import Tuple, List\n",
    "\n",
    "def termos_pg(a1: int, r: int, m: int) -> List[int]:\n",
    "    vals, v = [], a1\n",
    "    for _ in range(m):\n",
    "        vals.append(v)\n",
    "        v *= r\n",
    "    return vals\n",
    "\n",
    "def load_dataset_cfg(path: str, dataset: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = json.load(f)\n",
    "\n",
    "    if \"mongo\" not in cfg or \"uri\" not in cfg[\"mongo\"]:\n",
    "        raise ValueError(\"Config inválida: faltou mongo.uri\")\n",
    "\n",
    "    if \"datasets\" not in cfg or dataset not in cfg[\"datasets\"]:\n",
    "        raise ValueError(f\"Config inválida: faltou datasets.{dataset}\")\n",
    "\n",
    "    ds = cfg[\"datasets\"][dataset]\n",
    "    if \"batch_union\" not in ds:\n",
    "        raise ValueError(f\"Config inválida: faltou datasets.{dataset}.batch_union\")\n",
    "\n",
    "    b = ds[\"batch_union\"]\n",
    "    for key in [\"source_db\", \"part_prefix\", \"dest_db\", \"batch_prefix\", \"pg\", \"performance\"]:\n",
    "        if key not in b:\n",
    "            raise ValueError(f\"Config inválida: faltou batch_union.{key}\")\n",
    "    for key in [\"dn1\", \"r\", \"m\"]:\n",
    "        if key not in b[\"pg\"]:\n",
    "            raise ValueError(f\"Config inválida: faltou batch_union.pg.{key}\")\n",
    "    for key in [\"read_limit\", \"read_batch_size\", \"write_chunk_size\"]:\n",
    "        if key not in b[\"performance\"]:\n",
    "            raise ValueError(f\"Config inválida: faltou batch_union.performance.{key}\")\n",
    "\n",
    "    return {\n",
    "        \"MONGO_URI\": cfg[\"mongo\"][\"uri\"],\n",
    "        \"SOURCE_DB\": b[\"source_db\"],\n",
    "        \"PART_PREFIX\": b[\"part_prefix\"],\n",
    "        \"DEST_DB\": b[\"dest_db\"],\n",
    "        \"BATCH_PREFIX\": b[\"batch_prefix\"],\n",
    "        \"Dn1\": b[\"pg\"][\"dn1\"],\n",
    "        \"r\": b[\"pg\"][\"r\"],\n",
    "        \"m\": b[\"pg\"][\"m\"],\n",
    "        \"READ_LIMIT\": b[\"performance\"][\"read_limit\"],\n",
    "        \"READ_BATCH_SIZE\": b[\"performance\"][\"read_batch_size\"],\n",
    "        \"WRITE_CHUNK_SIZE\": b[\"performance\"][\"write_chunk_size\"],\n",
    "    }\n",
    "\n",
    "def copy_collection_by_pagination(db_src, coll_src: str, db_dst, coll_dst: str,\n",
    "                                  read_limit: int, read_batch: int, write_chunk: int) -> int:\n",
    "    db_dst[coll_dst].drop()\n",
    "    inserted = 0\n",
    "    last_id = None\n",
    "    while True:\n",
    "        q = {\"_id\": {\"$gt\": last_id}} if last_id is not None else {}\n",
    "        docs = list(\n",
    "            db_src[coll_src]\n",
    "            .find(q, no_cursor_timeout=False)\n",
    "            .sort([(\"_id\", 1)])\n",
    "            .limit(read_limit)\n",
    "            .batch_size(read_batch)\n",
    "        )\n",
    "        if not docs:\n",
    "            break\n",
    "        start = 0\n",
    "        n = len(docs)\n",
    "        while start < n:\n",
    "            end = min(start + write_chunk, n)\n",
    "            chunk = docs[start:end]\n",
    "            db_dst[coll_dst].insert_many(\n",
    "                chunk, ordered=False, bypass_document_validation=True\n",
    "            )\n",
    "            inserted += len(chunk)\n",
    "            start = end\n",
    "        last_id = docs[-1][\"_id\"]\n",
    "    return inserted\n",
    "\n",
    "def append_collection_by_pagination(db_src, coll_src: str, db_dst, coll_dst: str,\n",
    "                                    read_limit: int, read_batch: int, write_chunk: int) -> int:\n",
    "    appended = 0\n",
    "    last_id = None\n",
    "    while True:\n",
    "        q = {\"_id\": {\"$gt\": last_id}} if last_id is not None else {}\n",
    "        docs = list(\n",
    "            db_src[coll_src]\n",
    "            .find(q, no_cursor_timeout=False)\n",
    "            .sort([(\"_id\", 1)])\n",
    "            .limit(read_limit)\n",
    "            .batch_size(read_batch)\n",
    "        )\n",
    "        if not docs:\n",
    "            break\n",
    "        start = 0\n",
    "        n = len(docs)\n",
    "        while start < n:\n",
    "            end = min(start + write_chunk, n)\n",
    "            chunk = docs[start:end]\n",
    "            db_dst[coll_dst].insert_many(\n",
    "                chunk, ordered=False, bypass_document_validation=True\n",
    "            )\n",
    "            appended += len(chunk)\n",
    "            start = end\n",
    "        last_id = docs[-1][\"_id\"]\n",
    "    return appended\n",
    "\n",
    "def clone_same_db_with_out(db, src_coll: str, dst_coll: str) -> bool:\n",
    "    try:\n",
    "        db[dst_coll].drop()\n",
    "        db[src_coll].aggregate(\n",
    "            [{\"$match\": {}}, {\"$out\": dst_coll}],\n",
    "            allowDiskUse=True,\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[aviso] clone_same_db_with_out falhou ({src_coll} -> {dst_coll}): {e}\")\n",
    "        return False\n",
    "\n",
    "def resolve_args(default_config=\"config.json\", default_dataset=\"vk\"):\n",
    "    pos = [a for a in sys.argv[1:] if not a.startswith(\"-\")]\n",
    "    config_path = pos[0] if len(pos) >= 1 else default_config\n",
    "    dataset = pos[1] if len(pos) >= 2 else default_dataset\n",
    "    return config_path, dataset\n",
    "\n",
    "def main(config_path: str = \"config.json\", dataset: str = \"vk\"):\n",
    "    cfg = load_dataset_cfg(config_path, dataset)\n",
    "    MONGO_URI = cfg[\"MONGO_URI\"]\n",
    "    SOURCE_DB = cfg[\"SOURCE_DB\"]\n",
    "    PART_PREFIX = cfg[\"PART_PREFIX\"]\n",
    "    DEST_DB = cfg[\"DEST_DB\"]\n",
    "    BATCH_PREFIX = cfg[\"BATCH_PREFIX\"]\n",
    "    Dn1, r, m = cfg[\"Dn1\"], cfg[\"r\"], cfg[\"m\"]\n",
    "    READ_LIMIT = cfg[\"READ_LIMIT\"]\n",
    "    READ_BATCH_SIZE = cfg[\"READ_BATCH_SIZE\"]\n",
    "    WRITE_CHUNK_SIZE = cfg[\"WRITE_CHUNK_SIZE\"]\n",
    "\n",
    "    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
    "    client.admin.command(\"ping\")\n",
    "    src_db = client[SOURCE_DB]\n",
    "    dst_db = client[DEST_DB]\n",
    "\n",
    "    sizes = termos_pg(Dn1, r, m)\n",
    "    part_names = [f\"{PART_PREFIX}{s}\" for s in sizes]            # vk_450, vk_900, ...\n",
    "    batch_names = [f\"{BATCH_PREFIX}{i}\" for i in range(1, m+1)]  # vk_batch_1, ...\n",
    "\n",
    "    existing_src = set(src_db.list_collection_names())\n",
    "    missing = [c for c in part_names if c not in existing_src]\n",
    "    if missing:\n",
    "        raise RuntimeError(\n",
    "            f\"As coleções disjuntas não existem em '{SOURCE_DB}': {missing}. \"\n",
    "            \"Crie-as primeiro com o script de particionamento.\"\n",
    "        )\n",
    "\n",
    "    # P1 = Dn1\n",
    "    p1_src = part_names[0]\n",
    "    p1_dst = batch_names[0]\n",
    "    c1 = copy_collection_by_pagination(src_db, p1_src, dst_db, p1_dst,\n",
    "                                       READ_LIMIT, READ_BATCH_SIZE, WRITE_CHUNK_SIZE)\n",
    "    print(f\"P1: {SOURCE_DB}.{p1_src} -> {DEST_DB}.{p1_dst}  (copiados {c1} docs)\")\n",
    "\n",
    "    # P_i = P_{i-1} ∪ Dn_i\n",
    "    for i in range(2, m + 1):\n",
    "        prev_dst = batch_names[i-2]\n",
    "        out_dst = batch_names[i-1]\n",
    "        part_src = part_names[i-1]\n",
    "\n",
    "        cloned = clone_same_db_with_out(dst_db, prev_dst, out_dst)\n",
    "        if not cloned:\n",
    "            _ = copy_collection_by_pagination(dst_db, prev_dst, dst_db, out_dst,\n",
    "                                              READ_LIMIT, READ_BATCH_SIZE, WRITE_CHUNK_SIZE)\n",
    "\n",
    "        appended = append_collection_by_pagination(src_db, part_src, dst_db, out_dst,\n",
    "                                                   READ_LIMIT, READ_BATCH_SIZE, WRITE_CHUNK_SIZE)\n",
    "        total_i = dst_db[out_dst].estimated_document_count()\n",
    "        print(f\"P{i}: ({DEST_DB}.{prev_dst} ∪ {SOURCE_DB}.{part_src}) -> {DEST_DB}.{out_dst}  \"\n",
    "              f\"(anexados {appended}, total {total_i})\")\n",
    "\n",
    "#     for name in batch_names:\n",
    "#         cnt = dst_db[name].estimated_document_count()\n",
    "#         print(f\"{DEST_DB}.{name}: {cnt} documentos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg_path, ds = resolve_args()\n",
    "    # Em Jupyter, você também pode chamar: main(\"config.json\", \"vk\")\n",
    "    cfg_path=\"config.json\"\n",
    "    ds=\"vk\"\n",
    "    main(cfg_path, ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
