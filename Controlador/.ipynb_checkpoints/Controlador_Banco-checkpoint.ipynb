{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a2c687",
   "metadata": {},
   "source": [
    "# Controlador e Configuração de entrada\n",
    "\n",
    "O Controlador e a configuração de entrada são usados para gerar a carga de trabalho necessária para a nossa abordagem. O Controlador monitora a inserção de novos documentos na coleção e verifica se o número de novos documentos adicionados atingiu ou excedeu o limite pré-definido pelo usuário, estabelecido em termos da quantidade de novos documentos na coleção, determinando assim se uma atualização do esquema é necessária.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6597fe",
   "metadata": {},
   "source": [
    "![Descrição da imagem](caminho/para/sua/imagem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc0cfd",
   "metadata": {},
   "source": [
    "Monitoramentos Banco de dados MongoDB usando replicaset\n",
    "\n",
    "https://www.mongodb.com/developer/languages/python/python-change-streams/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d52acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import time\n",
    "import logging\n",
    "from bson.json_util import dumps\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Função para se conectar ao MongoDB\n",
    "def conectar_mongo():\n",
    "    return pymongo.MongoClient('mongodb://localhost:27016,localhost:27018,localhost:27019/?replicaSet=rs0')\n",
    "\n",
    "# Função para iniciar o changestream em uma coleção do MongoDB\n",
    "def iniciar_changestream(client, collection_name, resume_token=None):\n",
    "    return client.changestream[collection_name].watch(\n",
    "        [{'$match': {'operationType': {'$in': ['insert']}}}],\n",
    "        resume_after=resume_token\n",
    "    )\n",
    "\n",
    "# Função para salvar um documento em outro banco de dados MongoDB\n",
    "def salvar_documento_em_outro_banco(documento, colecao_destino):\n",
    "    try:\n",
    "        client_destino = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "        database_destino = client_destino['seu_outro_banco']\n",
    "        colecao_destino = database_destino[colecao_destino]\n",
    "        colecao_destino.insert_one(documento)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Erro ao salvar documento em outro banco: {e}')\n",
    "\n",
    "def acionar_evolucao (num_documentos, limite_documentos):\n",
    "    # Verificar se a contagem de documentos atingiu o limite definido pelo usuário\n",
    "    if num_documentos >= limite_documentos:\n",
    "        logger.info(f'Atingiu o limite de {limite_documentos} documentos.')\n",
    "        logger.info(f'Iniciando a atualização com {limite_documentos}')\n",
    "        \n",
    "        # Executar a atualização do esquema\n",
    "        # Usar com id\n",
    "        # executar_atualizacao_do_esquema (Id)\n",
    "        update = executar_atualizacao_do_esquema() \n",
    "        \n",
    "        if update:\n",
    "            logger.info('O esquema foi atualizado ...')\n",
    "            # Atualizando indice do documentos \n",
    "            indice_pa += 1\n",
    "            num_documentos = 0\n",
    "            atualizacao_do_esquema_concluida = False\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            logger.info('Esperando a atualização do esquema terminar...')\n",
    "            time.sleep(30)\n",
    "    else:\n",
    "        atualizacao_do_esquema_concluida = False\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91373bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Conecta-se ao MongoDB\n",
    "    client = conectar_mongo()\n",
    "    resume_token = None\n",
    "    primeiro_termo_pa = 100\n",
    "    razao_pa = 100\n",
    "    num_documentos = 0\n",
    "    indice_pa = 0\n",
    "    atualizacao_do_esquema_concluida = True\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Inicia o changestream na coleção especificada \n",
    "            # monitorando a coleção twitter_100000\n",
    "            with iniciar_changestream(client, 'twitter_100000', resume_token) as change_stream:\n",
    "                for change in change_stream:\n",
    "                    # Extrai o documento inserido e determina a coleção de destino\n",
    "                    documento_inserido = change['fullDocument']\n",
    "                    limite_documentos = primeiro_termo_pa + (indice_pa * razao_pa)\n",
    "                    \n",
    "                    # Em vez de usar colecao_ deve usar o nome da coleção que esta sendo monitorada. \n",
    "                    colecao_destino = f'colecao_{limite_documentos}'\n",
    "                    \n",
    "                    # Salva o documento em outro banco de dados\n",
    "                    salvar_documento_em_outro_banco(documento_inserido, colecao_destino)\n",
    "                    \n",
    "                    # Atualiza o token de continuação e a contagem de documentos\n",
    "                    #Salvar o token em um arquivo para recuperar caso seja necessario\n",
    "                    resume_token = change['_id']\n",
    "                    num_documentos += 1\n",
    "\n",
    "                    logger.info(f'num_documentos: {num_documentos}')\n",
    "                    logger.info(dumps(change))\n",
    "                    logger.info('')\n",
    "\n",
    "                    # Verifica se a contagem de documentos atingiu o limite\n",
    "                    if num_documentos >= limite_documentos:\n",
    "                        logger.info(f'Atingiu o limite de {limite_documentos} documentos.')\n",
    "                        logger.info(f'Iniciando a atualização com {limite_documentos} documentos...')\n",
    "                        \n",
    "                        # Executa a atualização do esquema\n",
    "                        update = executar_atualizacao_do_esquema()\n",
    "\n",
    "                        if update:\n",
    "                            logger.info('A atualização do esquema terminou... ')\n",
    "                            # Atualiza o índice e reinicia a contagem de documentos\n",
    "                            indice_pa += 1\n",
    "                            num_documentos = 0\n",
    "                            atualizacao_do_esquema_concluida = False\n",
    "                            time.sleep(2)\n",
    "                        else:\n",
    "                            logger.info('Esperando a atualização do esquema terminar...')\n",
    "                            time.sleep(30)\n",
    "                    else:\n",
    "                        atualizacao_do_esquema_concluida = False\n",
    "\n",
    "                # Verifica se a atualização do esquema foi concluída\n",
    "                if not atualizacao_do_esquema_concluida:\n",
    "                    logger.info('Atividades de atualização concluídas.')\n",
    "                    break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info('Encerrando programa devido a KeyboardInterrupt.')\n",
    "    except pymongo.errors.PyMongoError as e:\n",
    "        logger.error(f\"Erro no ChangeStream: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro inesperado: {e}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Ponto de entrada do script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c199df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878f06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002001f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a4527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c721867",
   "metadata": {},
   "source": [
    "### Requisição para evoluir esquema JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "\n",
    "\n",
    "# Função para realizar a requisição e medir o tempo de execução\n",
    "def make_request(url, data, headers):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Imprime as informações\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"Status code: {response.status_code}\")\n",
    "    \n",
    "    try:\n",
    "        # Tenta decodificar a resposta JSON\n",
    "        response_json = response.json()\n",
    "        print(f\"Response: {response_json}\")\n",
    "    except JSONDecodeError:\n",
    "        # Se houver um erro ao decodificar, imprime uma mensagem indicando que não foi possível decodificar a resposta\n",
    "        print(\"Response: Unable to decode JSON\")\n",
    "    \n",
    "    print(f\"Tempo de processamento: {elapsed_time} segundos\")\n",
    "    print(\"------------------------\")  # Adiciona uma linha para separar as saídas\n",
    "    \n",
    "    return response, elapsed_time\n",
    "\n",
    "\n",
    "# def make_request(url, data, headers):\n",
    "#     start_time = time.time()\n",
    "#     response = requests.post(url, json=data, headers=headers)\n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     return response, elapsed_time\n",
    "\n",
    "def run_iteration(url_batch, url_update, data_batch, headers_batch, collection_name, num_repetitions):\n",
    "    elapsed_times_batch = []\n",
    "    elapsed_times_update = []\n",
    "\n",
    "    with open('resultados.csv', mode='a', newline='') as file:\n",
    "        fieldnames = ['Iteração', 'Coleção', 'Tempo de Execução (Batch)', 'Tempo de Execução (Update)']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "        for iteration in range(num_repetitions):\n",
    "            response_batch, elapsed_time_batch = make_request(url_batch, data_batch, headers_batch)\n",
    "            elapsed_times_batch.append(elapsed_time_batch)\n",
    "\n",
    "            data_update = {\n",
    "                \"authentication\": {\n",
    "                    \"authMechanism\": \"SCRAM-SHA-1\",\n",
    "                    \"userName\": \"\",\n",
    "                    \"password\": \"\"\n",
    "                },\n",
    "                \"port\": \"27017\",\n",
    "                \"address\": \"localhost\",\n",
    "                \"databaseName\": \"experimentTwitterUpdate\",\n",
    "                \"collectionName\": collection_name,\n",
    "                \"userId\": \"655a51b42a775fdad3eab456\",\n",
    "                \"batchId\": response_batch.json().get('batchId')\n",
    "            }\n",
    "\n",
    "            response_update, elapsed_time_update = make_request(url_update, data_update, headers_batch)\n",
    "            elapsed_times_update.append(elapsed_time_update)\n",
    "\n",
    "            writer.writerow({\n",
    "                'Iteração': iteration + 1,\n",
    "                'Coleção': collection_name,\n",
    "                'Tempo de Execução (Batch)': elapsed_time_batch,\n",
    "                'Tempo de Execução (Update)': elapsed_time_update\n",
    "            })\n",
    "\n",
    "    return elapsed_times_batch, elapsed_times_update\n",
    "\n",
    "def calculate_average_times(elapsed_times_batch, elapsed_times_update, num_repetitions):\n",
    "    average_time_batch = sum(elapsed_times_batch) / num_repetitions\n",
    "    average_time_update = sum(elapsed_times_update) / num_repetitions\n",
    "\n",
    "    print(f\"Número de repetições: {num_repetitions}\")\n",
    "    print(f\"Tempo médio de processamento (batch): {average_time_batch:.6f} segundos\")\n",
    "    print(f\"Tempo médio de processamento (update): {average_time_update:.6f} segundos\")\n",
    "\n",
    "def executar_atualizacao_do_esquema():\n",
    "    # Configurações compartilhadas entre as requisições\n",
    "    url_batch = 'http://localhost:4200/api/batch/rawschema/steps/all'\n",
    "    url_update = 'http://localhost:4200/api/batch/rawschema/steps/allupdate'\n",
    "\n",
    "    # Dados do lote (batch) para twitter_400\n",
    "    data_batch = {\n",
    "        \"authentication\": {\n",
    "            \"authMechanism\": \"SCRAM-SHA-1\",\n",
    "            \"userName\": \"\",\n",
    "            \"password\": \"\"\n",
    "        },\n",
    "        \"port\": \"27017\",\n",
    "        \"address\": \"localhost\",\n",
    "        \"databaseName\": \"experimentTwitterBatchBB\",\n",
    "        \"collectionName\": \"experiment_twitter_batch_1\",\n",
    "        \"userId\": \"6557b923d761a2380847cb84\",\n",
    "        \"batchId\": \"65526db0989842e5e5d89e1d\"\n",
    "    }\n",
    "\n",
    "    # Cabeçalhos para a solicitação do lote (batch)\n",
    "    headers_batch = {\n",
    "        'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjp7Il9pZCI6IjY1ODEwYzYyY2YyMTY0Njk0NDUxNTM0NCIsInVzZXJuYW1lIjoiZWxlb25pbGlhIiwiZW1haWwiOiJhZG1pbkBnbWFpbC5jb20iLCJjcmVhdGVkQXQiOiIyMDIzLTEyLTE5VDAzOjIyOjEwLjQ4NFoiLCJ1cGRhdGVkQXQiOiIyMDIzLTEyLTE5VDAzOjIyOjEwLjQ4NFoiLCJfX3YiOjB9LCJpYXQiOjE3MDI5NTYxMzB9._ie_brzLETlLk0cUh7JK_nh-BDuOG7QDEsOZxkFs1-g',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Nome da coleção para atualização\n",
    "    collection_name = \"twitter_900\"\n",
    "\n",
    "    # Número de repetições\n",
    "    num_repetitions = 2\n",
    "\n",
    "    elapsed_times_batch, elapsed_times_update = run_iteration(url_batch, url_update, data_batch, headers_batch, collection_name, num_repetitions)\n",
    "    calculate_average_times(elapsed_times_batch, elapsed_times_update, num_repetitions)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92987182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33852274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
